{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, LSTM, Dropout, TimeDistributed, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5167\n"
     ]
    }
   ],
   "source": [
    "vocab = open('data/msr/msr_training_words.utf8').read().rstrip('\\n').split('\\n')\n",
    "vocab = list(''.join(vocab))\n",
    "stat = {}\n",
    "for v in vocab:\n",
    "    stat[v] = stat.get(v, 0) + 1\n",
    "stat = sorted(stat.items(), key=lambda x:x[1], reverse=True)\n",
    "vocab = [s[0] for s in stat]\n",
    "print(len(vocab))\n",
    "\n",
    "char2id = {c : i + 1 for i, c in enumerate(vocab)}\n",
    "id2char = {i+1 : c for i, c in enumerate(vocab)}\n",
    "tags = {'s' : 0, 'b' : 1, 'm' : 2, 'e' : 3, 'x' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "maxlen = 32 # 长于32则截断， 短于32则填充0\n",
    "hidden_size = 64\n",
    "batch_size = 64\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = open(path).read().rstrip('\\n')\n",
    "    # 按标点符号和换行符分隔\n",
    "    data = re.split('[，。！？、\\n]', data)\n",
    "    print('共有数据 %d 条' % len(data))\n",
    "    print('平均长度：', np.mean([len(d.replace(' ', '')) for d in data]))\t\n",
    "    \n",
    "    # 准备数据\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        \n",
    "        sentence = sentence.split(' ')\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        try:\n",
    "            for s in sentence:\n",
    "                s = s.strip()\n",
    "                # 跳过空字符\n",
    "                if len(s) == 0:\n",
    "                    continue\n",
    "                # s\n",
    "                elif len(s) == 1:\n",
    "                    X.append(char2id[s])\n",
    "                    y.append(tags['s'])\n",
    "                elif len(s) > 1:\n",
    "                    # b\n",
    "                    X.append(char2id[s[0]])\n",
    "                    y.append(tags['b'])\n",
    "                    # m\n",
    "                    for i in range(1, len(s) - 1):\n",
    "                        X.append(char2id[s[i]])\n",
    "                        y.append(tags['m'])\n",
    "                    # e\n",
    "                    X.append(char2id[s[-1]])\n",
    "                    y.append(tags['e'])\n",
    "            \n",
    "            # 统一长度\n",
    "            if len(X) > maxlen:\n",
    "                X = X[:maxlen]\n",
    "                y = y[:maxlen]\n",
    "            else:\n",
    "                for i in range(maxlen - len(X)):\n",
    "                    X.append(0)\n",
    "                    y.append(tags['x'])\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            if len(X) > 0:\n",
    "                X_data.append(X)\n",
    "                y_data.append(y)\n",
    "    print(y_data[0:3])\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np_utils.to_categorical(y_data, 5)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有数据 385152 条\n",
      "平均长度： 9.742236831173146\n",
      "[[0, 1, 3, 0, 0, 1, 3, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 2, 2, 3, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [0, 1, 3, 0, 1, 2, 2, 3, 0, 0, 0, 0, 1, 3, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]\n",
      "共有数据 17961 条\n",
      "平均长度： 9.48605311508268\n",
      "[[1, 3, 1, 3, 0, 0, 1, 3, 1, 3, 0, 1, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [1, 3, 0, 1, 3, 1, 3, 0, 1, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]\n",
      "X_train size: (385152, 32)\n",
      "y_train size: (385152, 32, 5)\n",
      "X_test size: (17917, 32)\n",
      "y_test size: (17917, 32, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data('data/msr/msr_training.utf8')\n",
    "X_test, y_test = load_data('data/msr/msr_test_gold.utf8')\n",
    "print('X_train size:', X_train.shape)\n",
    "print('y_train size:', y_train.shape)\n",
    "print('X_test size:', X_test.shape)\n",
    "print('y_test size:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基于lstm进行训练\n",
    "X = Input(shape=(maxlen,), dtype='int32')\n",
    "# 转化成词向量\n",
    "embedding = Embedding(input_dim=len(vocab) + 1, output_dim=embedding_size, input_length=maxlen, mask_zero=True)(X)\n",
    "# Bidirectional 双向\n",
    "blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(embedding)\n",
    "blstm = Dropout(0.6)(blstm)\n",
    "blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(blstm)\n",
    "blstm = Dropout(0.6)(blstm)\n",
    "output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n",
    "\n",
    "model = Model(X, output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "model.save('data/msr_bilstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.evaluate(X_train, y_train, batch_size=batch_size))\n",
    "print(model.evaluate(X_test, y_test, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用动态规划算法获得最大概率路径\n",
    "def viterbi(nodes):\n",
    "    trans = {'be': 0.5, 'bm': 0.5, 'eb': 0.5, 'es': 0.5, 'me': 0.5, 'mm': 0.5, 'sb': 0.5, 'ss': 0.5}\n",
    "    paths = {'b': nodes[0]['b'], 's': nodes[0]['s']}\n",
    "    for l in range(1, len(nodes)):\n",
    "        paths_ = paths.copy()\n",
    "        paths = {}\n",
    "        for i in nodes[l].keys():\n",
    "            nows = {}\n",
    "            for j in paths_.keys():\n",
    "                if j[-1] + i in trans.keys():\n",
    "                    nows[j + i] = paths_[j] + nodes[l][i] + trans[j[-1] + i]\n",
    "            nows = sorted(nows.items(), key=lambda x: x[1], reverse=True)\n",
    "            paths[nows[0][0]] = nows[0][1]\n",
    "    \n",
    "    paths = sorted(paths.items(), key=lambda x: x[1], reverse=True)\n",
    "    return paths[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_words(data):\n",
    "    data = re.split('[，。！？、\\n]', data)\n",
    "    sens = []\n",
    "    Xs = []\n",
    "    for sentence in data:\n",
    "        sen = []\n",
    "        X = []\n",
    "        sentence = list(sentence)\n",
    "        for s in sentence:\n",
    "            s = s.strip()\n",
    "            if not s == '' and s in char2id:\n",
    "                sen.append(s)\n",
    "                X.append(char2id[s])\n",
    "        if len(X) > maxlen:\n",
    "            sen = sen[:maxlen]\n",
    "            X = X[:maxlen]\n",
    "        else:\n",
    "            for i in range(maxlen - len(X)):\n",
    "                X.append(0)\n",
    "        \n",
    "        if len(sen) > 0:\n",
    "            Xs.append(X)\n",
    "            sens.append(sen)\n",
    "    \n",
    "    Xs = np.array(Xs)\n",
    "    ys = model.predict(Xs)\n",
    "    \n",
    "    results = ''\n",
    "    for i in range(ys.shape[0]):\n",
    "        nodes = [dict(zip(['s', 'b', 'm', 'e'], d[:4])) for d in ys[i]]\n",
    "        ts = viterbi(nodes)\n",
    "        for x in range(len(sens[i])):\n",
    "            if ts[x] in ['s', 'e']:\n",
    "                results += sens[i][x] + '/'\n",
    "            else:\n",
    "                results += sens[i][x]\n",
    "        \n",
    "    return results[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
